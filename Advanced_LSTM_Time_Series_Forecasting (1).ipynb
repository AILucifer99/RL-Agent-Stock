{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hkJjjNNxn7iT",
        "uSO9RgwqoUj0",
        "Vdq2HOTzDBJE",
        "T5vCHicRCyp7",
        "CVV-zJiRCnoj",
        "kZ60XICHBvft",
        "0g2HBfy9B0K0",
        "U8vwDoIACD8l",
        "njVMd4JkCZkl",
        "t-L7Gy1_Cerk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the required libraries for the forecasting system."
      ],
      "metadata": {
        "id": "hkJjjNNxn7iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance --quiet"
      ],
      "metadata": {
        "id": "EWDkGgF0SIAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the required module. "
      ],
      "metadata": {
        "id": "uSO9RgwqoUj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class StockData :\n",
        "    def __init__(self, name_of_stock, start_date, end_date, data_type=\"csv\") :\n",
        "        super(StockData, self).__init__()\n",
        "        self. name_of_stock = name_of_stock\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.data_type = data_type\n",
        "\n",
        "    def getStockDataToCSV(self, **kwargs) :\n",
        "        if self.data_type == \"csv\" :\n",
        "            stock_data = yf.download(self.name_of_stock, start=self.start_date, end=self.end_date)\n",
        "            if kwargs[\"verbose\"] > -1 : \n",
        "                print(\"{}The Data is : {}\".format(\"\\n\", \"\\n\"))\n",
        "                print(stock_data.head(kwargs[\"num_samples\"]))\n",
        "            result = stock_data.to_csv(kwargs[\"file_name\"], encoding=kwargs[\"file_encoding\"], index=True)\n",
        "            return True\n",
        "        else :\n",
        "            print(\"Only CSV file is supported, change the data_type....\")\n",
        "            return False\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    STOCK = \"AAPL\"\n",
        "    START = \"2018-08-11\" # Date in the format of yyyy-mm-dd\n",
        "    END = \"2019-08-11\" # Date in the format of yyyy-mm-dd\n",
        "    FILE_NAME = \"{}-{}.csv\".format(STOCK, END)\n",
        "    VERBOSE = -2\n",
        "    NUM_SAMPLES = 15\n",
        "    ENCODING = \"utf-8\"\n",
        "\n",
        "    stock = StockData(STOCK, START, END)\n",
        "    result = stock.getStockDataToCSV(num_samples=NUM_SAMPLES, \n",
        "                                     verbose=VERBOSE, \n",
        "                                     file_name=FILE_NAME, \n",
        "                                     file_encoding=ENCODING)\n",
        "    if result :\n",
        "        print(\"Data Generated Successfully....\")\n",
        "    else :\n",
        "        print(\"Internal Exception occured try again....\")\n",
        "        print(\"Try again....\")\n"
      ],
      "metadata": {
        "id": "NEqwFRYKlZKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code for the Stock Prediction System starts here."
      ],
      "metadata": {
        "id": "3r4wt0frzTxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset accessing with the help of Yahoo Finance API "
      ],
      "metadata": {
        "id": "Vdq2HOTzDBJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "stock_data = yf.download('AAPL', start='2015-08-11', end='2022-08-11')\n",
        "stock_data.head()"
      ],
      "metadata": {
        "id": "_1JratqNzgOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = stock_data.to_csv(\"APPL-2019.csv\", encoding='utf-8', index=True)"
      ],
      "metadata": {
        "id": "AubTRvfvkhqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "xV-4XZKymj6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stock_data)"
      ],
      "metadata": {
        "id": "DmtP94ypRWH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Stock Prices History for Open Values')\n",
        "plt.plot(stock_data[\"Open\"])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Prices ($)')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Stock Prices History for Close Values')\n",
        "plt.plot(stock_data[\"Close\"], color=\"y\")\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Prices ($)')\n"
      ],
      "metadata": {
        "id": "2n33f2rz76r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preprocessing and training and validation tensor generation."
      ],
      "metadata": {
        "id": "T5vCHicRCyp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "close_prices = stock_data['Close']\n",
        "num_previous_days = 150\n",
        "\n",
        "values = close_prices.values\n",
        "training_data_len = math.ceil(len(values)* 0.8)\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = values[training_data_len:]\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(values.reshape(-1,1))\n",
        "\n",
        "train_data = scaled_data[0: training_data_len, :]\n",
        "\n",
        "for i in range(num_previous_days, len(train_data)):\n",
        "    x_train.append(train_data[i-num_previous_days:i, 0])\n",
        "    y_train.append(train_data[i, 0])\n",
        "    \n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "\n",
        "test_data = scaled_data[training_data_len-num_previous_days: , : ]\n",
        "\n",
        "for i in range(num_previous_days, len(test_data)):\n",
        "    x_test.append(test_data[i-num_previous_days:i, 0])\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "PFIUECa6TIyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The shapes of the final training tensors are : {} and {}\".format(x_train.shape, y_train.shape))\n",
        "print(\"The shapes of the validation tensors are : {} and {}\".format(x_test.shape, y_test.shape))"
      ],
      "metadata": {
        "id": "51Vz0FBpTidQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a class of LSTM Neural Network for the Analysis of the Stocks Market Prices"
      ],
      "metadata": {
        "id": "CVV-zJiRCnoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings     \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "class LSTMNeuralNetwork :\n",
        "    def __init__(self, num_lstm_layers, lstm_config, learning_rate, optimizer_type) :\n",
        "        super(LSTMNeuralNetwork, self).__init__()\n",
        "        self.num_lstm_layers = num_lstm_layers\n",
        "        self.lstm_config = lstm_config\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer_type = optimizer_type\n",
        "\n",
        "    def CuDnnLSTMNetwork(self, **kwargs) :\n",
        "        model = tf.keras.models.Sequential()\n",
        "        model.add(tf.compat.v1.keras.layers.CuDNNLSTM(kwargs[\"num_cudnn_units\"], \n",
        "                                                      return_sequences=True, \n",
        "                                                      input_shape=(x_train.shape[1], 1)))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.Dropout(rate=kwargs[\"layer_dropout\"]))\n",
        "\n",
        "        model.add(tf.compat.v1.keras.layers.CuDNNLSTM(units = (kwargs[\"num_cudnn_units\"] + \\\n",
        "                                                               kwargs[\"internal_cudnn_embedding\"])))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.Dropout(rate=kwargs[\"layer_dropout\"] + kwargs[\"internal_dropout_embedding\"]))\n",
        "        \n",
        "        # Multi-Layered Perceptron\n",
        "        model.add(tf.keras.layers.Dense(kwargs[\"num_mlp_neurons\"]))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.27))\n",
        "        model.add(tf.keras.layers.Dropout(rate=kwargs[\"mlp_dropout\"]))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        if self.optimizer_type == \"adam\" :\n",
        "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), \n",
        "                        loss='mean_squared_error', metrics=[\"mse\"])\n",
        "            return model\n",
        "        elif self.optimizer_type == \"rmsprop\" :\n",
        "            model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate, \n",
        "                                                                momentum=0.45), \n",
        "                          loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
        "            return model\n",
        "        elif self.optimizer_type == \"nadam\" :\n",
        "            model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=self.learning_rate), \n",
        "                          loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
        "            return model\n",
        "\n",
        "    def CompoundLSTMNetwork(self, **kwargs) :\n",
        "        model = tf.keras.models.Sequential()\n",
        "        model.add(tf.keras.layers.LSTM(kwargs[\"units\"], return_sequences=True, \n",
        "                                       input_shape=(x_train.shape[1], 1)))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.27))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        model.add(tf.keras.layers.LSTM(kwargs[\"units\"] + kwargs[\"lstm_embedding\"], \n",
        "                                       return_sequences=False, \n",
        "                                       dropout=kwargs[\"lstm_dropout\"]))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.27))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(kwargs[\"num_mlp_neurons\"]))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.27))\n",
        "        model.add(tf.keras.layers.Dropout(rate=kwargs[\"mlp_dropout\"]))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        if self.optimizer_type == \"adam\" :\n",
        "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), \n",
        "                        loss='mean_squared_error', metrics=[\"mse\"])\n",
        "            return model\n",
        "        elif self.optimizer_type == \"rmsprop\" :\n",
        "            model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate, \n",
        "                                                                momentum=0.45), \n",
        "                          loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
        "            return model\n",
        "        elif self.optimizer_type == \"nadam\" :\n",
        "            model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=self.learning_rate), \n",
        "                          loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
        "            return model\n",
        "\n",
        "    def StackedLSTMNetwork(self, **kwargs) :\n",
        "        init = tf.keras.initializers.glorot_normal(seed=None)\n",
        "        init1 = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05)\n",
        "        input_tensor = x = tf.keras.layers.Input(shape=(x_train.shape[1], 1))\n",
        "        \n",
        "        for i in range(self.num_lstm_layers) :\n",
        "            name = 'layer_lstm_{0}'.format(i+1)\n",
        "            if ( (i == 0) and (self.num_lstm_layers == 1) ):\n",
        "                x = tf.keras.layers.LSTM(units=self.lstm_config[0], \n",
        "                                         dropout=kwargs[\"dropout\"], \n",
        "                                         recurrent_dropout=kwargs[\"recurrent_dropout\"], \n",
        "                        return_sequences=False, kernel_initializer=init, \n",
        "                        activation=kwargs[\"activation\"], name=name)(x)\n",
        "            elif (i != (self.num_lstm_layers - 1) ) :\n",
        "                x = tf.keras.layers.LSTM(units=self.lstm_config[1], \n",
        "                                         dropout=kwargs[\"dropout\"], \n",
        "                                         recurrent_dropout=kwargs[\"recurrent_dropout\"],  \n",
        "                        return_sequences=True, kernel_initializer=init, \n",
        "                        activation=kwargs[\"activation\"], name=name)(x)\n",
        "            else:\n",
        "                x = tf.keras.layers.LSTM(units=self.lstm_config[2], \n",
        "                                         dropout=kwargs[\"dropout\"], \n",
        "                                         recurrent_dropout=kwargs[\"recurrent_dropout\"],  \n",
        "                        return_sequences=False, kernel_initializer=init, \n",
        "                        activation=kwargs[\"activation\"], name=name)(x)\n",
        "        \n",
        "        # Multi-Layered Perceptron Block\n",
        "        x = tf.keras.layers.Dense(250)(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(rate=0.20)(x)\n",
        "        x = tf.keras.layers.Dense(50)(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(rate=0.10)(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(1, activation='linear', kernel_initializer= init1)(x)\n",
        "\n",
        "        model = tf.keras.models.Model(input_tensor, x)\n",
        "        if self.optimizer_type == \"rmsprop\" :\n",
        "            optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate, rho=0.9, epsilon=None, decay=0.0, momentum=0.45)\n",
        "            model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
        "            return model\n",
        "        elif self.optimizer_type == \"adam\" :\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "            model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mse\"])\n",
        "            return model\n",
        "        elif self.optimizer_type == \"nadam\" :\n",
        "            optimizer = tf.keras.optimizers.Nadam(learning_rate=self.learning_rate)\n",
        "            model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mse\"])\n",
        "            return model\n",
        "        else :\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "            model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mse\"])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "\n",
        "    # Hyper-Parameters for the Neural Network Class\n",
        "    num_internal_lstm_layers = 2\n",
        "    network_configuration = [100, 200, 100]\n",
        "    learningRate = 0.00025\n",
        "    optimizerType = \"adam\"\n",
        "    \n",
        "    # Training hyper-parameters of the system.\n",
        "    num_epochs = 50\n",
        "    batch_size = 128\n",
        "    validation_split = 0.2\n",
        "\n",
        "    # Stacked LSTM Hyper-parameters (**kwargs)\n",
        "    stacked_lstm_dropout = 0.25\n",
        "    stacked_lstm_recurrent_dropout = 0.25\n",
        "    stacked_lstm_activation = \"sigmoid\"\n",
        "\n",
        "    # Compound LSTM Hyper-Parameters (**kwargs)\n",
        "    compound_lstm_units = 150\n",
        "    compound_lstm_embedding = 150\n",
        "    compound_lstm_dropout = 0.25\n",
        "    num_mlp_units_compound_lstm = 256\n",
        "    mlp_dropout_compound_lstm = 0.15\n",
        "\n",
        "    # CuDnn LSTM Hyper-Parameters (**kwargs)\n",
        "    cudnn_units = 275\n",
        "    cudnn_layer_dropout = 0.15\n",
        "    cudnn_embedding_dim = 150\n",
        "    cudnn_dropout_embedding = 0.15\n",
        "    cudnn_mlp_units = 512\n",
        "    cudnn_mlp_dropout = 0.2\n",
        "\n",
        "\n",
        "    # Initialization of the LSTM Neural Network Class\n",
        "    nn = LSTMNeuralNetwork(num_internal_lstm_layers, network_configuration, \n",
        "                            learningRate, optimizerType)\n",
        "\n",
        "    # The First Model\n",
        "    model_1 = nn.StackedLSTMNetwork(dropout=stacked_lstm_dropout, \n",
        "                                        recurrent_dropout=stacked_lstm_recurrent_dropout, \n",
        "                                        activation=stacked_lstm_activation)\n",
        "    print(\"{}The Stacked LSTM Internal Model Architectural Details Are : {}\".format(\"\\n\", \"\\n\"))\n",
        "    print(model_1.summary())\n",
        "\n",
        "    print(\"\\n\")\n",
        "    # The Second Model\n",
        "    model_2 = nn.CompoundLSTMNetwork(units=compound_lstm_units, \n",
        "                                        lstm_embedding=compound_lstm_embedding, \n",
        "                                        lstm_dropout=compound_lstm_dropout, \n",
        "                                        num_mlp_neurons=num_mlp_units_compound_lstm, \n",
        "                                        mlp_dropout=mlp_dropout_compound_lstm)\n",
        "    print(\"{}The Compound LSTM Internal Model Architectural Details Are : {}\".format(\"\\n\", \"\\n\"))\n",
        "    print(model_2.summary())\n",
        "\n",
        "    model_cudnn = nn.CuDnnLSTMNetwork(num_cudnn_units=cudnn_units, \n",
        "                                        layer_dropout=cudnn_layer_dropout, \n",
        "                                        internal_cudnn_embedding=cudnn_embedding_dim, \n",
        "                                        internal_dropout_embedding=cudnn_dropout_embedding, \n",
        "                                        num_mlp_neurons=cudnn_mlp_units, \n",
        "                                        mlp_dropout=cudnn_mlp_dropout)\n",
        "    print(\"{}The CUDNNLSTM Internal Model Architectural Details Are : {}\".format(\"\\n\", \"\\n\"))\n",
        "    print(model_cudnn.summary())\n",
        "\n"
      ],
      "metadata": {
        "id": "td-dP3v7WuSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CuDnn-LSTM Model Training "
      ],
      "metadata": {
        "id": "kZ60XICHBvft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_cudnn = model_cudnn.fit(x_train, \n",
        "                                y_train, \n",
        "                                batch_size=batch_size if batch_size is not None else 16, \n",
        "                                epochs=num_epochs if num_epochs is not None else 20, \n",
        "                                validation_split=validation_split if validation_split is not None else 0.15, \n",
        "                                shuffle=True)"
      ],
      "metadata": {
        "id": "8Etb3JJG-ghz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Stacked-LSTM Model Training"
      ],
      "metadata": {
        "id": "0g2HBfy9B0K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = model_1.fit(x_train, \n",
        "                        y_train, \n",
        "                        batch_size=batch_size if batch_size is not None else 16, \n",
        "                        epochs=num_epochs if num_epochs is not None else 20, \n",
        "                        validation_split=validation_split if validation_split is not None else 0.15, \n",
        "                        shuffle=True)"
      ],
      "metadata": {
        "id": "YT6npq5MlON0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compound-LSTM Model Training"
      ],
      "metadata": {
        "id": "U8vwDoIACD8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_2 = model_2.fit(x_train, \n",
        "                        y_train, \n",
        "                        batch_size=batch_size if batch_size is not None else 32, \n",
        "                        epochs=num_epochs if num_epochs is not None else 75, \n",
        "                        validation_split=validation_split if validation_split is not None else 0.15, \n",
        "                        shuffle=True)"
      ],
      "metadata": {
        "id": "TGm4rZc7lvnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The three plots of the model's training and validation."
      ],
      "metadata": {
        "id": "njVMd4JkCZkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for first model loss\n",
        "plt.plot(history_cudnn.history['loss'])\n",
        "plt.plot(history_cudnn.history['val_loss'])\n",
        "plt.title('CuDnn LSTM Model Loss Graph')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for second model loss\n",
        "plt.plot(history_1.history['loss'])\n",
        "plt.plot(history_1.history['val_loss'])\n",
        "plt.title('Stacked LSTM Model Loss Graph')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for second model loss\n",
        "plt.plot(history_2.history['loss'])\n",
        "plt.plot(history_2.history['val_loss'])\n",
        "plt.title('Compound LSTM Model Loss Graph')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZVTKhr9cVkU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Validation of the trained models on the testing dataset."
      ],
      "metadata": {
        "id": "t-L7Gy1_Cerk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_cudnn = model_cudnn.predict(x_test)\n",
        "predictions_cudnn = scaler.inverse_transform(predictions_cudnn)\n",
        "rmse = np.sqrt(np.mean(predictions_1 - y_test) ** 2)\n",
        "print(\"The Validation RMSE of the designed model is : {}\".format(rmse))\n"
      ],
      "metadata": {
        "id": "ksnbNnabDLx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_1 = model_1.predict(x_test)\n",
        "predictions_1 = scaler.inverse_transform(predictions_1)\n",
        "rmse = np.sqrt(np.mean(predictions_1 - y_test) ** 2)\n",
        "print(\"The Validation RMSE of the designed model is : {}\".format(rmse))\n"
      ],
      "metadata": {
        "id": "1XKYmXMjUhSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_2 = model_2.predict(x_test)\n",
        "predictions_2 = scaler.inverse_transform(predictions_2)\n",
        "rmse = np.sqrt(np.mean(predictions_2 - y_test) ** 2)\n",
        "print(\"The Validation RMSE of the designed model is : {}\".format(rmse))\n"
      ],
      "metadata": {
        "id": "z-V3153Tt_Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = stock_data.filter(['Close'])\n",
        "train = data[:training_data_len]\n",
        "validation = data[training_data_len:]\n",
        "validation['Predictions'] = predictions_cudnn\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Model')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price USD ($)')\n",
        "plt.plot(train)\n",
        "plt.plot(validation[['Close', 'Predictions']])\n",
        "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CEMFE560DYKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = stock_data.filter(['Close'])\n",
        "train = data[:training_data_len]\n",
        "validation = data[training_data_len:]\n",
        "validation['Predictions'] = predictions_1\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Model')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price USD ($)')\n",
        "plt.plot(train)\n",
        "plt.plot(validation[['Close', 'Predictions']])\n",
        "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qhM25wWNWJsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = stock_data.filter(['Close'])\n",
        "train = data[:training_data_len]\n",
        "validation = data[training_data_len:]\n",
        "validation['Predictions'] = predictions_2\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Model')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price USD ($)')\n",
        "plt.plot(train)\n",
        "plt.plot(validation[['Close', 'Predictions']])\n",
        "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8W7V7ztuuPIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6RIQCT7ziUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}